import cv2
import numpy as np
from dwpose import DwposeDetector
import torch
import torchvision
import tkinter as tk
from PIL import Image, ImageTk
import torch.nn as nn

def get_keypoints(image_rgb):
    # DWpose로 keypoints 추출
    image_rgb = image_rgb
    imgOut,j,source = model(image_rgb,
    include_hand=True,
    include_face=True,
    include_body=True,
    image_and_json=True,
    detect_resolution=512)

    # error_pass = False
    # pose_kpts = None
    # hand_left = None
    # hand_right =None
    # face = None
    # try:
    #     if j['people'][1]:
    #        pose_kpts = np.zeros((18, 3), dtype=np.float32)
    #        hand_left = np.zeros((21, 3), dtype=np.float32)
    #        hand_right = np.zeros((21, 3), dtype=np.float32)
    #        face = np.zeros((70, 3), dtype=np.float32)
    #        error_pass = True 
    # except:
    #     try:
    #         pose_kpts = np.array(j['people'][0]['pose_keypoints_2d'])
    #     except:
    #         pass
    #     if pose_kpts is None:
    #         pose_kpts = np.zeros((18, 3), dtype=np.float32)
    #         error_pass = True
    #     elif pose_kpts.size == 18 * 3:
    #         pose_kpts = pose_kpts.reshape(18, 3)
    #     else:
    #         pose_kpts = np.zeros((18, 3), dtype=np.float32)
    #         error_pass = True
        
    #     try:
    #         hand_left = np.array(j['people'][0]['hand_left_keypoints_2d'], dtype=np.float32)
    #     except:
    #         pass
    #     if hand_left is None:
    #         hand_left = np.zeros((21, 3), dtype=np.float32)
    #     elif hand_left.size == 21 * 3:
    #         hand_left = hand_left.reshape(21, 3)
    #     else:
    #         hand_left = np.zeros((21, 3), dtype=np.float32)
        
    #     try:
    #         hand_right = np.array(j['people'][0]['hand_right_keypoints_2d'], dtype=np.float32)
    #     except:
    #         pass
    #     if hand_right is None:
    #         hand_right = np.zeros((21, 3), dtype=np.float32)
    #     elif hand_right.size == 21 * 3:
    #         hand_right = hand_right.reshape(21, 3)
    #     else:
    #         hand_right = np.zeros((21, 3), dtype=np.float32)
            
    #     try:
    #         face = np.array(j['people'][0]['face_keypoints_2d'], dtype=np.float32)
    #     except:
    #         pass
    #     if face is None:
    #         face = np.zeros((70, 3), dtype=np.float32)
    #         error_pass = True
    #     elif face.size == 70 * 3:
    #         face = face.reshape(70, 3)
    #     else:
    #         face = np.zeros((70, 3), dtype=np.float32)
    #         error_pass = True
    # kpts = np.concatenate([pose_kpts, hand_left, hand_right, face], axis=0)  # (130, 3)
    # def keypoints_to_heatmaps(keypoints, height, width, h2, w2, sigma=1):
    #     heatmaps = np.zeros((keypoints.shape[0], height, width), dtype=np.uint8)  #130, 128, 256
    #     for idx, (x, y, v) in enumerate(keypoints):
    #         if v < 0.05:
    #             continue
    #         x = int(x*width/w2)
    #         y = int(y*height/h2)
    #         if x < 0 or y < 0 or x >= width or y >= height:
    #             continue
            
    #         cv2.circle(heatmaps[idx], (x, y), sigma, 1, -1)
    #     return torch.tensor(heatmaps, dtype=torch.uint8)  # [127, H, W]

    # h, w = image_rgb.shape[:2]
    # h2 = np.array(imgOut).shape[0]
    # w2 = np.array(imgOut).shape[1]

    # pose_tensor_heatmap = keypoints_to_heatmaps(kpts, h, w, h2, w2)*255
    out_pose = resize(torch.from_numpy(np.array(imgOut)).permute(2, 0, 1))  # [3, 128, 256]
    image_rgb_tensor = torch.from_numpy(image_rgb).permute(2, 0, 1)
    return image_rgb_tensor, out_pose
def conv1x1(in_channels, out_channels):
    return nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)


def conv3x3(in_channels, out_channels):
    return nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)


def downconv2x(in_channels, out_channels):
    return nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)


def upconv2x(in_channels, out_channels):
    return nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)


class ResidualBlock(nn.Module):
    
    def __init__(self, num_channels):
        super(ResidualBlock, self).__init__()
        layers = [
            conv3x3(num_channels, num_channels),
            nn.BatchNorm2d(num_channels),
            nn.ReLU(inplace=True),
            conv3x3(num_channels, num_channels),
            nn.BatchNorm2d(num_channels)
        ]
        self.layers = nn.Sequential(*layers)
    
    def forward(self, x):
        y = self.layers(x) + x
        return y


class Generator(nn.Module):
    
    def __init__(self, in1_channels, in2_channels, out_channels, ngf=64):
        super().__init__()
        self.in1_conv1 = self.inconv(in1_channels, ngf)
        self.in1_down1 = self.down2x(ngf, ngf*2)
        self.in1_down2 = self.down2x(ngf*2, ngf*4)
        self.in1_down3 = self.down2x(ngf*4, ngf*8)
        self.in1_down4 = self.down2x(ngf*8, ngf*16)
        
        self.in2_conv1 = self.inconv(in2_channels, ngf)
        self.in2_down1 = self.down2x(ngf, ngf*2)
        self.in2_down2 = self.down2x(ngf*2, ngf*4)
        self.in2_down3 = self.down2x(ngf*4, ngf*8)
        self.in2_down4 = self.down2x(ngf*8, ngf*16)
        
        self.out_up1 = self.up2x(ngf*16, ngf*8)
        self.out_up2 = self.up2x(ngf*8, ngf*4)
        self.out_up3 = self.up2x(ngf*4, ngf*2)
        self.out_up4 = self.up2x(ngf*2, ngf)
        self.out_conv1 = self.outconv(ngf, out_channels)
    
    def inconv(self, in_channels, out_channels):
        return nn.Sequential(
            conv3x3(in_channels, out_channels),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def outconv(self, in_channels, out_channels):
        return nn.Sequential(
            ResidualBlock(in_channels),
            ResidualBlock(in_channels),
            ResidualBlock(in_channels),
            ResidualBlock(in_channels),
            conv1x1(in_channels, out_channels),
            nn.Tanh()
        )
    
    def down2x(self, in_channels, out_channels):
        return nn.Sequential(
            downconv2x(in_channels, out_channels),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            ResidualBlock(out_channels)
        )
    
    def up2x(self, in_channels, out_channels):
        return nn.Sequential(
            upconv2x(in_channels, out_channels),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            ResidualBlock(out_channels)
        )
    
    def forward(self, origin_img, pose_img, origin_pose_img):
        
        pose_img=torch.cat([pose_img, origin_pose_img], dim =1 )
        
        x1_c1 = self.in1_conv1(origin_img)
        x1_d1 = self.in1_down1(x1_c1)
        x1_d2 = self.in1_down2(x1_d1)
        x1_d3 = self.in1_down3(x1_d2)
        x1_d4 = self.in1_down4(x1_d3)
        
        x2_c1 = self.in2_conv1(pose_img)
        x2_d1 = self.in2_down1(x2_c1)
        x2_d2 = self.in2_down2(x2_d1)
        x2_d3 = self.in2_down3(x2_d2)
        x2_d4 = self.in2_down4(x2_d3)
        
        y = x1_d4 * torch.sigmoid(x2_d4)
        y = self.out_up1(y)
        y = y * torch.sigmoid(x2_d3)
        y = self.out_up2(y)
        y = y * torch.sigmoid(x2_d2)
        y = self.out_up3(y)
        y = y * torch.sigmoid(x2_d1)
        y = self.out_up4(y)
        y = self.out_conv1(y)
        return y
    
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(device)

gen = torch.load('./pth/train6gen840001.pt', weights_only=False)  

model = DwposeDetector.from_pretrained_default()
resize = torchvision.transforms.Resize((144, 256))

root = tk.Tk()
root.title("Generated Images")
img_label1 = tk.Label(root)
img_label2 = tk.Label(root)
img_label3 = tk.Label(root)
img_label4 = tk.Label(root)
img_label5 = tk.Label(root)
img_label6 = tk.Label(root)
img_label1.grid(row=0, column=0)
img_label2.grid(row=0, column=1)
img_label3.grid(row=1, column=0)
img_label4.grid(row=1, column=1)
img_label5.grid(row=2, column=1)
img_label6.grid(row=2, column=0)


    
image_bgr = cv2.imread('00000.jpg')
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
image_rgb = cv2.resize(image_rgb, (256, 144))
image_rgb_tensor_origin, out_pose_origin = get_keypoints(image_rgb)

cap = cv2.VideoCapture(0)
#1920 x 1080      x : 1080 = 114 : 256     123120 = 256x  x = 480.93   640 : 480     x:480 = 114 : 256   54720  =256x   x = 213.75
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 256)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 144)
if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()
while True:
    ret, frame = cap.read()
    frame = cv2.flip(frame, 1)
    # frame = frame[0:480, 213: 213+214]
    
    if not ret:
        print("Error: Failed to capture frame.")
        break
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame = cv2.resize(frame, (256, 144))

    image_rgb_tensor, out_pose = get_keypoints(frame)
    

    origin_image_tensor = image_rgb_tensor_origin.to(device).type(torch.float)/ 127.5 - 1.0
    pose_tensor = out_pose.to(device).type(torch.float)/ 127.5 - 1.0
    pose_tensor_img= out_pose_origin.to(device).type(torch.float)/ 127.5 - 1.0
    generated_images = gen(origin_image_tensor.unsqueeze(0), pose_tensor.unsqueeze(0), pose_tensor_img.unsqueeze(0))

    pil_image1 = Image.fromarray(((origin_image_tensor+ 1) * 127.5).permute(1,2,0).detach().cpu().numpy().astype(np.uint8))
    tk_image1 = ImageTk.PhotoImage(pil_image1)
    img_label1.config(image=tk_image1)
    img_label1.image = tk_image1

    generated_image_np = ((generated_images[0]+ 1) * 127.5).permute(1,2,0).detach().cpu().numpy()
    generated_image_np = generated_image_np  # Rescale to [0, 255]
    generated_image_np = generated_image_np.astype(np.uint8)
    pil_image2 = Image.fromarray(generated_image_np)
    tk_image2 = ImageTk.PhotoImage(pil_image2)
    img_label2.config(image=tk_image2)
    img_label2.image = tk_image2


    pil_image5 = Image.fromarray(image_rgb_tensor.permute(1,2,0).detach().cpu().numpy().astype(np.uint8))
    tk_image5 = ImageTk.PhotoImage(pil_image5)
    img_label5.config(image=tk_image5)
    img_label5.image = tk_image5
                        

    generated_image_np = out_pose.permute(1,2,0).detach().cpu().numpy()
    generated_image_np = generated_image_np  # Rescale to [0, 255]
    generated_image_np = generated_image_np.astype(np.uint8)
    pil_image6 = Image.fromarray(generated_image_np)
    tk_image6 = ImageTk.PhotoImage(pil_image6)
    img_label6.config(image=tk_image6)
    img_label6.image = tk_image6
                        
    root.update()
